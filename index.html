<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ËµµËç£Ëáª</title>
  <!-- Bootstrap 5 CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Link to my CSS file -->
  <link href="style.css" rel="stylesheet">
  <!-- Bootstrap 5 JS Bundle (includes Popper) -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <!-- MathJax, support LaTex -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-dark bg-dark sticky-top navbar-expand-lg">
    <div class="container">
      <a class="navbar-brand font-fs" href="index.html"><b>ËµµËç£Ëáª</b></a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#main-nav"
        aria-controls="main-nav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="main-nav">
        <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
          <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="#news">News</a></li>
          <li class="nav-item"><a class="nav-link" href="#research">Research</a></li>
          <li class="nav-item"><a class="nav-link" href="#publication">Publication</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Content -->
  <div class="container mt-4">
    <div class="row">
      <!-- Sidebar -->
      <aside class="col-md-3 mb-4">
        <div class="card">
          <div class="card-body text-center">
            <img src="portrait.jpg" alt="Portrait" class="img-fluid rounded mb-3">
            <h3><strong>ZHAO, Rongzhen</strong></h3>
            <p class="mt-2">
              Doctoral Researcher<br>
              Aalto University<br>
              <code><a href="mailto:rongzhen.zhao@aalto.fi">rongzhen.zhao@aalto.fi</a></code>
            </p>
            <p>
              Google Scholar:
              <code><a
                href="https://scholar.google.com/citations?hl=en&user=MqlwrKAAAAAJ&view_op=list_works&sortby=pubdate">MqlwrKAAAAAJ</a></code><br>
              GitHub: <code><a href="https://github.com/Genera1Z?tab=repositories">Genera1Z</a></code><br>
              LinkedIn:
              <code><a href="https://www.linkedin.com/in/rongzhen-zhao-3b7215247">rongzhen-zhao-3b7215247</a></code><br>
              WeChat: <code>Genera1Z</code>
            </p>
          </div>
        </div>
      </aside>

      <!-- Main content -->
      <main class="col-md-9">
        <div class="card mb-4">
          <div class="card-body">
            <section id="about">
              <h2>About</h2>
              <p>
                I am a Doctoral Researcher at <a href="https://www.aalto.fi/en/aalto-university/rankings">Aalto
                  University</a>.
                I am supervised by Professor <a href="https://scholar.google.com/citations?user=-2fJStwAAAAJ">Joni
                  Pajarinen</a>, who leads our <a href="https://rl.aalto.fi">Aalto Robot Learning Lab</a>.
                My co-supervisors are Professor <a
                  href="https://scholar.google.com/citations?user=i2gcTBQAAAAJ">Alexander
                  Ilin</a> and Professor <a href="https://scholar.google.com/citations?user=c4mWQPQAAAAJ">Juho
                  Kannala</a>.
              </p>
              <p>
                My research is in the domain of Computer Vision, focusing on <b>Object-Centric</b> representation <b>Learning</b> (OCL).
                By combining OCL with mainstream World Models (WM), I aim to enhance embodied agents' Perception, Understanding,
                Reasoning and Prediction of visual scenes, as well as their Planning, Decision-making and Acting in environments.
              </p>
              <p>
                I have devoted a lot to writing my <b>object-centric-bench</b> framework, where many OCL methods are implemented with unified,
                strong training tricks. Thus all those well-known OCL methods can be evaluated and compared under fair
                settings. This framework has already been adopted by many follower works. Take my own as an example:
                <a href="https://github.com/Genera1Z/VQ-VFM-OCL">VQ-VFM-OCL</a>,
                <a href="https://github.com/Genera1Z/DIAS">DIAS</a>,
                <a href="https://github.com/Genera1Z/RandSF.Q">RandSF.Q</a>,
                and <a href="https://github.com/Genera1Z/SmoothSA">SmoothSA</a>.
              </p>
              <p>
                I am lookings for <b>academic collaborators</b>.
                If you are interested to apply OCL to tasks like visual/video question answering, visual
                prediction/reasoning, world modeling and reinforcement learning, please do not hesitate to contact me.
              </p>
              <p>
                I provide <b>academic supervision for Bachelor and Master students</b>, free of charge.
                I have too many ideas to explore due to limited time.
                Contact me DIRECTLY with your academic RESUME if you are interested in working with me.
                <b>Ideas</b> and <b>GPUs</b> (with my supervisor's permission) will not be a problem.
              <ul>
                <li> I have helped a Master student publish a <a href="https://arxiv.org/abs/2505.20772">NeurIPS 2025
                    paper</a></li>
                <li>I have also helped another Master student with a top conference submission</li>
              </ul>
              </p>
            </section>
          </div>
        </div>

        <div class="card mb-4">
          <div class="card-body">
            <section id="news">
              <h2>News</h2>
              <ul>
                <li>[2025/11/08] My paper was accepted to AAAI 2026 üéâ</li>
                <li>[2025/09/19] My Master student Hongjia's paper was accepted to NeurIPS 2025 üéâ</li>
                <li>[2025/07/04] My two papers were accepted to ACM MM 2025 üéâ</li>
              </ul>
            </section>
          </div>
        </div>

        <div class="card mb-4">
          <div class="card-body">
            <section id="research">
              <h2>Research</h2>
              <p>
                OCL aims to represent image/video as (sub-)object-level feature vectors, termed slots.
                OCL models follow the encode-aggregate-decode architecture, trained by reconstructing the input in some form.
              </p>
              <p>I have been improving OCL in its aggregation, decoding and reconstruction, as well as its transition.</p>
              <p>I am trying to combine OCL with mainstream WMs.</p>
              <p>I am trying to improve OCL with stronger geometry inductive bias by combining OCL with geometric vision models, like NeRF and GS.</p>
            </section>
          </div>
        </div>

        <div class="card mb-4">
          <div class="card-body">
            <section id="publication">
              <h2>Publication</h2>

              <b>Preprint</b>
              <ul>
                <li>G Li‚Ä†, R Zhao‚Ä†*, J Deng, Y Wang, J Pajarinen.
                  Object-Centric Vision Token Pruning for Vision Language Models.
                  arXiv:2511.20439. <br>
                  <code>[OC-VTP]<a href="https://github.com/GarryLarry010131/OC-VTP">code</a></code> <br>
                  üåüüåü <span style="color: gray">guaranteed optimal vision token pruning for the first time, by combining OCL</span>
                </li>
                <li>R Zhao, W Yang, J Kannala, J Pajarinen.
                  Smoothing Slot Attention Iterations and Recurrences.
                  arXiv:2508.05417. <br>
                  <code>[SmoothSA]<a href="https://github.com/Genera1Z/SmoothSA">code/model/log</a></code>
                  üåüüåü <span style="color: gray">new SotA of OCL on both images and videos</span>
                </li>
              </ul>
              
              <b>2026</b>
              <ul>
                <li>R Zhao, J Li, J Kannala, J Pajarinen.
                  Predicting Video Slot Attention Queries from Random Slot-Feature Pairs.
                  AAAI 2026. <br>
                  <code>[RandSF.Q]<a href="https://github.com/Genera1Z/RandSF.Q">code/model/log</a></code>
                  üåüüåüüåü <span style="color: gray">implicit transition dynamics modeling in video OCL for the first time; significant performance gains</span>
                </li>
              </ul>

              <b>2025</b>
              <ul>
                <li>H Liu, R Zhao*, H Chen, J Pajarinen.
                  Break Through the Fixed Number of Slots in Object-Centric Learning.
                  NeurIPS 2025. <br>
                  <code>[MetaSlot]<a href="https://github.com/lhj-lhj/MetaSlot">code</a></code>
                </li>
                <li>R Zhao, Y Zhao, J Kannala, J Pajarinen.
                  Slot Attention with Re-Initialization and Self-Distillation.
                  ACM MM 2025. <br>
                  <code>[DIAS]<a href="https://github.com/Genera1Z/DIAS">code/model/log</a></code>
                </li>
                <li>R Zhao, V Wang, J Kannala, J Pajarinen.
                  Vector-Quantized Vision Foundation Models for Object-Centric Learning.
                  ACM MM 2025. <br>
                  <code>[VQ-VFM-OCL]<a href="https://github.com/Genera1Z/VQ-VFM-OCL">code/model/log</a></code>
                  üåüüåü <span style="color: gray">unify mainstream OCL by supporting any decoding; reproduce many baselines</span>
                </li>
                <li>R Zhao, V Wang, J Kannala, J Pajarinen.
                  Multi-Scale Fusion for Object Representation.
                  ICLR 2025. <br>
                  <code>[MSF]<a href="https://github.com/Genera1Z/MultiScaleFusion">code/model</a></code>
                </li>
                <li>R Zhao, V Wang, J Kannala, J Pajarinen.
                  Grouped Discrete Representation for Object-Centric Learning.
                  ECML-PKDD 2025. <br>
                  <code>[GDR]<a href="https://github.com/Genera1Z/GroupedDiscreteRepresentation">code/model</a></code>
                </li>
              </ul>
            </section>
          </div>
        </div>
      </main>
    </div>
  </div>

</body>

</html>
