<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ËµµËç£Ëáª</title>
  <!-- Bootstrap 5 CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Link to my CSS file -->
  <link href="style.css" rel="stylesheet">
  <!-- Bootstrap 5 JS Bundle (includes Popper) -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <!-- MathJax, support LaTex -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-dark bg-dark sticky-top navbar-expand-lg">
    <div class="container">
      <a class="navbar-brand font-fs" href="index.html"><b>ËµµËç£Ëáª</b></a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#main-nav"
        aria-controls="main-nav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="main-nav">
        <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
          <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="#news">News</a></li>
          <li class="nav-item"><a class="nav-link" href="#research">Research</a></li>
          <li class="nav-item"><a class="nav-link" href="#supervision">Supervision</a></li>
          <li class="nav-item"><a class="nav-link" href="#publication">Publication</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Content -->
  <div class="container mt-4">
    <div class="row">
      <!-- Sidebar -->
      <aside class="col-md-3 mb-4">
        <div class="card">
          <div class="card-body text-center">
            <img src="portrait.jpg" alt="Portrait" class="img-fluid rounded mb-3">
            <h3><strong>ZHAO, Rongzhen</strong></h3>
            <p class="mt-2">
              Doctoral Researcher<br>
              Aalto University<br>
              <code><a href="mailto:rongzhen.zhao@aalto.fi">rongzhen.zhao@aalto.fi</a></code>
            </p>
            <p>
              Google Scholar:
              <code><a
                href="https://scholar.google.com/citations?hl=en&user=MqlwrKAAAAAJ&view_op=list_works&sortby=pubdate">MqlwrKAAAAAJ</a></code><br>
              GitHub: <code><a href="https://github.com/Genera1Z?tab=repositories">Genera1Z</a></code><br>
              LinkedIn:
              <code><a href="https://www.linkedin.com/in/rongzhen-zhao-3b7215247">rongzhen-zhao-3b7215247</a></code><br>
            </p>
          </div>
        </div>
      </aside>

      <!-- Main content -->
      <main class="col-md-9">
        <div class="card mb-4">
          <div class="card-body">
            <section id="about">
              <h2>About</h2>
              <p>
                I am a Doctoral Researcher at <a href="https://www.aalto.fi/en/aalto-university/rankings">Aalto
                  University</a>, funded by <a href="https://fcai.fi/fcai-teams">FCAI</a> (Finnish Center for Artificial
                Intelligence).
                My primary supervisor is Professor <a href="https://scholar.google.com/citations?user=-2fJStwAAAAJ">Joni
                  Pajarinen</a>, who leads our <a href="https://rl.aalto.fi">Aalto Robot Learning Lab</a>.
                My co-supervisors are Professor <a
                  href="https://scholar.google.com/citations?user=i2gcTBQAAAAJ">Alexander
                  Ilin</a> and Professor <a href="https://scholar.google.com/citations?user=c4mWQPQAAAAJ">Juho
                  Kannala</a>.
              </p>
              <p>
                My research is in the domain of Computer Vision, focusing on <b>Object-Centric</b> representation
                <b>Learning</b> (OCL).
                By combining OCL with mainstream World Models (WM), I aim to enhance embodied agents' Perception,
                Understanding,
                Reasoning and Prediction of visual scenes, as well as their Planning, Decision-making and Acting in
                environments.
              </p>
              <p>
                I have devoted a lot to writing my <b><code>object-centric-bench</code></b> framework, where many OCL methods are
                implemented with unified,
                strong training tricks. Thus all those well-known OCL methods can be evaluated and compared under fair
                settings. This framework has already been adopted by many follower works. Take my own as an example:
                <a href="https://github.com/Genera1Z/VQ-VFM-OCL">VQ-VFM-OCL</a>,
                <a href="https://github.com/Genera1Z/DIAS">DIAS</a>,
                <a href="https://github.com/Genera1Z/RandSF.Q">RandSF.Q</a>,
                and <a href="https://github.com/Genera1Z/SmoothSA">SmoothSA</a>.
              </p>
              <p>
                I am looking for <b>academic collaborators</b>.
                If you are interested to apply OCL to tasks like visual/video question answering, visual
                prediction/reasoning, world modeling and reinforcement learning, please do not hesitate to contact me.
              </p>
              <p>
                I provide <b>academic supervision for Bachelor and Master students</b>, free of charge.
                I have too many ideas to explore due to limited time.
                Contact me DIRECTLY with your academic RESUME if you are interested in working with me.
                <b>Ideas</b> and <b>GPUs</b> (with my supervisor's permission) will not be a problem.
              <ul>
                <li> I have helped a Master student publish a <a href="https://arxiv.org/abs/2505.20772">NeurIPS 2025
                    paper</a></li>
                <li>I have also helped another Master student with a top conference submission</li>
              </ul>
              </p>
            </section>
          </div>
        </div>

        <div class="card mb-4">
          <div class="card-body">
            <section id="news">
              <h2>News</h2>
              <ul>
                <li>[2025/11/08] My paper was accepted to AAAI 2026 üéâ</li>
                <li>[2025/09/19] My Master student Hongjia's paper was accepted to NeurIPS 2025 üéâ</li>
                <li>[2025/07/04] My two papers were accepted to ACM MM 2025 üéâ</li>
              </ul>
            </section>
          </div>
        </div>

        <div class="card mb-4">
          <div class="card-body">
            <section id="research">
              <h2>Research</h2>
              <p>
                OCL aims to represent image/video as (sub-)object-level feature vectors, termed slots.
                OCL models follow the encode-aggregate-decode architecture, trained by reconstructing the input in some
                form.
              </p>
              <p>I have been improving OCL in its aggregation, decoding and reconstruction, as well as its transition.
              </p>
              <p>I am trying to combine OCL with mainstream WMs.</p>
              <p>I am trying to improve OCL with stronger geometry inductive bias by combining OCL with geometric vision
                models, like NeRF and GS.</p>
            </section>
          </div>
        </div>

        <div class="card mb-4">
          <div class="card-body">
            <section id="supervision">
              <h2>Supervision</h2>

              <b>Current</b>
              <ul>
                <li>
                  Linda Liljavirta, 2026/01~Now. Bachelor student @ Aalto University. <br>
                  Literature review on 3D scene understanding, e.g., LangSplat.
                </li>
                <li>
                  Louis French (with Professor <a href="https://chengwang12.github.io">Cheng Wang</a>), 2025/12~Now. Bachelor student @ University of East Anglia. <br>
                  OCL for more efficient 3D/4D reconstruction, e.g., CUT3R.
                </li>
                <li>
                  Guangyuan Li, 2025/09~Now. Master student @ Aalto University. <br>
                  OCL for vision token pruning in VLMs and for context token pruning in LLMs. <br>
                  Output: <code><a href="https://arxiv.org/abs/2511.20439">OC-VTP</a></code>
                </li>
                <li>
                  Fan Chen (with Professor <a href="https://zhaobinnku.github.io">Bin Zhao</a>), 2025/05~Now. Master student @ Guilin University of Electronic Technology. <br>
                  OCL plus geometric computer vision.
                </li>
                <li>
                  Yanhua Han (with Professor <a href="https://zhaobinnku.github.io">Bin Zhao</a>), 2025/05~Now. Master student @ Guilin University of Electronic Technology. <br>
                  OCL for learning visual hierarchies.
                </li>
                <li>
                  Youliang Tao (with Professor <a href="https://zhaobinnku.github.io">Bin Zhao</a>), 2025/05~Now. Bachelor student @ Guilin University of Electronic Technology. <br>
                  OCL decoding with masked auto-encoding.
                </li>
              </ul>

              <b>Past</b>
              <ul>
                <li>
                  Janina Kemppainen, 2025/02~2025/05. Bachelor student @ Aalto University. <br>
                  No/low-code AI app building platform case study. <br>
                  Output:
                  <code><a href="https://aaltodoc.aalto.fi/items/2445e512-9ede-4bb0-988e-28da53efb7f0">thesis</a></code>
                </li>
                <li>
                  Hongjia Liu, 2024/12~2025/05. Master student @ Aalto University. <br>
                  OCL plus vector-quantization. <br>
                  Output: <code><a href="https://arxiv.org/abs/2505.20772">MetaSlot</a></code>
                </li>
                <li>
                  Daniel Kopra, 2023/02~2023/05. Bachelor student @ Aalto University. <br>
                  Literature review on modular design in deep neural networks. <br>
                  Output:
                  <code><a href="https://aaltodoc.aalto.fi/items/61a2a654-9627-4d71-a241-2652b543f502">thesis</a></code>
                </li>
              </ul>

            </section>
          </div>
        </div>

        <div class="card mb-4">
          <div class="card-body">
            <section id="publication">
              <h2>Publication</h2>

              <p>
                ': equal contribution; *: corresponding author
              </p>

              <b>Preprint</b>
              <ul>
                <li>R Zhao, W Yang, J Kannala, J Pajarinen.
                  Smoothing Slot Attention Iterations and Recurrences.
                  arXiv:2508.05417. <br>
                  <code>[SmoothSA]<a href="https://github.com/Genera1Z/SmoothSA">paper/code/model/log</a></code> <br>
                  üåüüåü <span style="color: gray">new SotA of OCL on both images and videos</span>
                </li>
              </ul>

              <b>2026</b>
              <ul>
                <li>G Li', R Zhao'*, J Deng, Y Wang, J Pajarinen.
                  Object-Centric Vision Token Pruning for Vision Language Models.
                  CVPR 2026 Findings. <br>
                  <code>[OC-VTP]<a href="https://github.com/GarryLarry010131/OC-VTP">paper/code</a></code> <br>
                  üåüüåü <span style="color: gray">guaranteed optimal vision token pruning for the first time, by
                    combining OCL</span>
                </li>
                <li>R Zhao, J Li, J Kannala, J Pajarinen.
                  Predicting Video Slot Attention Queries from Random Slot-Feature Pairs.
                  AAAI 2026. <br>
                  <code>[RandSF.Q]<a href="https://github.com/Genera1Z/RandSF.Q">paper/poster/code/model/log</a></code> <br>
                  üåüüåüüåü <span style="color: gray">implicit transition dynamics modeling in video OCL for the first
                    time; significant performance gains</span>
                </li>
              </ul>

              <b>2025</b>
              <ul>
                <li>H Liu, R Zhao*, H Chen, J Pajarinen.
                  Break Through the Fixed Number of Slots in Object-Centric Learning.
                  NeurIPS 2025. <br>
                  <code>[MetaSlot]<a href="https://github.com/lhj-lhj/MetaSlot">paper/code</a></code>
                </li>
                <li>R Zhao, Y Zhao, J Kannala, J Pajarinen.
                  Slot Attention with Re-Initialization and Self-Distillation.
                  ACM MM 2025. <br>
                  <code>[DIAS]<a href="https://github.com/Genera1Z/DIAS">paper/code/model/log</a></code>
                </li>
                <li>R Zhao, V Wang, J Kannala, J Pajarinen.
                  Vector-Quantized Vision Foundation Models for Object-Centric Learning.
                  ACM MM 2025. <br>
                  <code>[VQ-VFM-OCL]<a href="https://github.com/Genera1Z/VQ-VFM-OCL">paper/code/model/log</a></code> <br>
                  üåüüåü <span style="color: gray">unify mainstream OCL by supporting any decoding; reproduce many
                    baselines</span>
                </li>
                <li>R Zhao, V Wang, J Kannala, J Pajarinen.
                  Multi-Scale Fusion for Object Representation.
                  ICLR 2025. <br>
                  <code>[MSF]<a href="https://github.com/Genera1Z/MultiScaleFusion">paper/code/model</a></code>
                </li>
                <li>R Zhao, V Wang, J Kannala, J Pajarinen.
                  Grouped Discrete Representation for Object-Centric Learning.
                  ECML-PKDD 2025. <br>
                  <code>[GDR]<a href="https://github.com/Genera1Z/GroupedDiscreteRepresentation">paper/code/model</a></code>
                </li>
              </ul>
            </section>
          </div>
        </div>
      </main>
    </div>
  </div>

</body>

</html>
